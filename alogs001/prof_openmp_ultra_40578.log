==========================================
SLURM_CLUSTER_NAME = paramutkarsh
SLURM_ARRAY_JOB_ID = 
SLURM_ARRAY_TASK_ID = 
SLURM_ARRAY_TASK_COUNT = 
SLURM_ARRAY_TASK_MAX = 
SLURM_ARRAY_TASK_MIN = 
SLURM_JOB_ACCOUNT = cdac
SLURM_JOB_ID = 40578
SLURM_JOB_NAME = prof_omp_u
SLURM_JOB_NODELIST = cn031
SLURM_JOB_USER = chuk355
SLURM_JOB_UID = 23411
SLURM_JOB_PARTITION = cpu
SLURM_TASK_PID = 47583
SLURM_SUBMIT_DIR = /home/chuk355/HPC-NBody
SLURM_CPUS_ON_NODE = 48
SLURM_NTASKS = 1
SLURM_TASK_PID = 47583
==========================================
============================================
Profiling OpenMP Ultra
Particles: 1000 | Threads: 48
Output: vtune_reports/openmp_ultra_N1000_ID40578
============================================
--- Topology Check ---
Threads: 48
Thread 0 is running on Core 0 (NUMA Split Check)
Thread 24 is running on Core 24 (NUMA Split Check)
Time: 1.99276 s
GFLOPs: 10.0363
Elapsed Time: 4.700s
    CPU Time: 95.968s
        Effective Time: 91.860s
            Idle: 0s
            Poor: 2.180s
            Ok: 37.550s
            Ideal: 52.130s
            Over: 0s
        Spin Time: 4.098s
            Imbalance or Serial Spinning: 4.098s
            Lock Contention: 0s
            Other: 0s
        Overhead Time: 0.010s
            Creation: 0.010s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 48
    Paused Time: 0s

Top Hotspots
Function                    Module              CPU Time
--------------------------  ------------------  --------
main._omp_fn.1              nbody_openmp_ultra   91.760s
gomp_team_barrier_wait_end  libgomp.so.1          3.450s
gomp_simple_barrier_wait    libgomp.so.1          0.388s
gomp_team_barrier_wait      libgomp.so.1          0.200s
GOMP_single_start           libgomp.so.1          0.100s
[Others]                    N/A                   0.070s
Effective CPU Utilization: 42.2%
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Average Effective CPU Utilization: 20.274 out of 48
Collection and Platform Info
    Application Command Line: ./bin/nbody_openmp_ultra "1000" "1000" "bench" 
    Operating System: 3.10.0-1160.el7.x86_64 \S Kernel \r on an \m 
    Computer Name: cn031
    Result Size: 5.6 MB 
    Collection start time: 11:45:28 26/12/2025 UTC
    Collection stop time: 11:45:32 26/12/2025 UTC
    Collector Type: Driverless Perf per-process counting,User-mode sampling and tracing
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 2.893 GHz 
        Logical CPU Count: 48
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
