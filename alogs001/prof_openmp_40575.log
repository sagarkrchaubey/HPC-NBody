==========================================
SLURM_CLUSTER_NAME = paramutkarsh
SLURM_ARRAY_JOB_ID = 
SLURM_ARRAY_TASK_ID = 
SLURM_ARRAY_TASK_COUNT = 
SLURM_ARRAY_TASK_MAX = 
SLURM_ARRAY_TASK_MIN = 
SLURM_JOB_ACCOUNT = cdac
SLURM_JOB_ID = 40575
SLURM_JOB_NAME = prof_omp
SLURM_JOB_NODELIST = cn031
SLURM_JOB_USER = chuk355
SLURM_JOB_UID = 23411
SLURM_JOB_PARTITION = cpu
SLURM_TASK_PID = 46729
SLURM_SUBMIT_DIR = /home/chuk355/HPC-NBody
SLURM_CPUS_ON_NODE = 48
SLURM_NTASKS = 1
SLURM_TASK_PID = 46729
==========================================
============================================
Profiling OpenMP (Standard)
Particles: 5000 | Threads: 48
Output: vtune_reports/openmp_N5000_ID40575
============================================

--- N-Body OpenMP Simulation ---
Bodies: 5000 | Steps: 1000
Threads: 48
Output File: nbody_output_openmp_N5000_S1000_T48.csv

========================================================
                PERFORMANCE REPORT                      
========================================================
Threads Active:   48
Unique Cores Used: 48

--- Per-Thread Breakdown ---
ThreadID  Compute Time(s)Interactions(Ops)   Est. GFLOPs    
0         3.5609         524895000           2.9481         
1         3.4856         524895000           3.0118         
2         3.5176         524895000           2.9844         
3         3.4838         524895000           3.0133         
4         3.5265         524895000           2.9768         
5         3.5295         524895000           2.9743         
6         3.4811         524895000           3.0157         
7         3.5266         524895000           2.9768         
8         3.4861         519896000           2.9827         
9         3.5321         519896000           2.9438         
10        3.5299         519896000           2.9457         
11        3.5250         519896000           2.9498         
12        3.4869         519896000           2.9820         
13        3.5303         519896000           2.9453         
14        3.5327         519896000           2.9434         
15        3.5341         519896000           2.9422         
16        3.5347         519896000           2.9416         
17        3.4815         519896000           2.9866         
18        3.5319         519896000           2.9440         
19        3.5335         519896000           2.9427         
20        3.5316         519896000           2.9442         
21        3.4839         519896000           2.9845         
22        3.5318         519896000           2.9441         
23        3.5313         519896000           2.9445         
24        3.4633         519896000           3.0023         
25        3.4665         519896000           2.9996         
26        3.4850         519896000           2.9836         
27        3.4651         519896000           3.0008         
28        3.5150         519896000           2.9582         
29        3.5019         519896000           2.9692         
30        3.5010         519896000           2.9700         
31        3.5172         519896000           2.9563         
32        3.4686         519896000           2.9977         
33        3.4675         519896000           2.9987         
34        3.5142         519896000           2.9588         
35        3.5131         519896000           2.9598         
36        3.4984         519896000           2.9722         
37        3.5156         519896000           2.9577         
38        3.5162         519896000           2.9572         
39        3.4828         519896000           2.9855         
40        3.4786         519896000           2.9891         
41        3.4661         519896000           2.9999         
42        3.4936         519896000           2.9763         
43        3.5168         519896000           2.9567         
44        3.4659         519896000           3.0001         
45        3.4679         519896000           2.9984         
46        3.5029         519896000           2.9684         
47        3.5006         519896000           2.9703         

--- Overall Summary ---
Wall Clock Time:   11.2085 s
Combined GFLOPs:   44.6001 GFLOPs
========================================================
Elapsed Time: 15.215s
    CPU Time: 372.299s
        Effective Time: 165.549s
            Idle: 0.030s
            Poor: 165.519s
            Ok: 0s
            Ideal: 0s
            Over: 0s
        Spin Time: 206.730s
         | A significant portion of CPU time is spent waiting. Use this metric
         | to discover which synchronizations are spinning. Consider adjusting
         | spin wait parameters, changing the lock implementation (for example,
         | by backing off then descheduling), or adjusting the synchronization
         | granularity.
         |
            Imbalance or Serial Spinning: 206.730s
             | The threading runtime function related to time spent on imbalance
             | or serial spinning consumed a significant amount of CPU time.
             | This can be caused by a load imbalance, insufficient concurrency
             | for all working threads, or busy waits of worker threads while
             | serial code is executed. If there is an imbalance, apply dynamic
             | work scheduling or reduce the size of work chunks or tasks. If
             | there is insufficient concurrency, consider collapsing the outer
             | and inner loops. If there is a wait for completion of serial
             | code, explore options for parallelization with Intel Advisor,
             | algorithm, or microarchitecture tuning of the application's
             | serial code with VTune Profiler Basic Hotspots or
             | Microarchitecture Exploration analysis respectively. For OpenMP*
             | applications, use the Per-Barrier OpenMP Potential Gain metric
             | set in the HPC Performance Characterization analysis to discover
             | the reason for high imbalance or serial spin time.
             |
            Lock Contention: 0s
            Other: 0s
        Overhead Time: 0.020s
            Creation: 0.020s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 48
    Paused Time: 0s

Top Hotspots
Function                    Module        CPU Time
--------------------------  ------------  --------
gomp_simple_barrier_wait    libgomp.so.1  193.441s
update_physics._omp_fn.0    nbody_openmp  157.619s
gomp_team_barrier_wait_end  libgomp.so.1   12.550s
std::ostream::operator<<    nbody_openmp    1.760s
std::ostream::operator<<    nbody_openmp    1.600s
[Others]                    N/A             5.329s
Effective CPU Utilization: 49.9%
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Average Effective CPU Utilization: 23.961 out of 48
Collection and Platform Info
    Application Command Line: ./bin/nbody_openmp "5000" "1000" "bench" 
    Operating System: 3.10.0-1160.el7.x86_64 \S Kernel \r on an \m 
    Computer Name: cn031
    Result Size: 11.8 MB 
    Collection start time: 11:43:05 26/12/2025 UTC
    Collection stop time: 11:43:20 26/12/2025 UTC
    Collector Type: Driverless Perf per-process counting,User-mode sampling and tracing
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 2.893 GHz 
        Logical CPU Count: 48
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
