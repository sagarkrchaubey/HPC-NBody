==========================================
SLURM_CLUSTER_NAME = paramutkarsh
SLURM_ARRAY_JOB_ID = 
SLURM_ARRAY_TASK_ID = 
SLURM_ARRAY_TASK_COUNT = 
SLURM_ARRAY_TASK_MAX = 
SLURM_ARRAY_TASK_MIN = 
SLURM_JOB_ACCOUNT = cdac
SLURM_JOB_ID = 40580
SLURM_JOB_NAME = prof_omp_u
SLURM_JOB_NODELIST = cn052
SLURM_JOB_USER = chuk355
SLURM_JOB_UID = 23411
SLURM_JOB_PARTITION = cpu
SLURM_TASK_PID = 48946
SLURM_SUBMIT_DIR = /home/chuk355/HPC-NBody
SLURM_CPUS_ON_NODE = 48
SLURM_NTASKS = 1
SLURM_TASK_PID = 48946
==========================================
============================================
Profiling OpenMP Ultra
Particles: 10000 | Threads: 48
Output: vtune_reports/openmp_ultra_N10000_ID40580
============================================
--- Topology Check ---
Threads: 48
Thread 0 is running on Core 0 (NUMA Split Check)
Thread 24 is running on Core 24 (NUMA Split Check)
Time: 1.5781 s
GFLOPs: 1267.35
Elapsed Time: 4.278s
    CPU Time: 76.218s
        Effective Time: 71.829s
            Idle: 0s
            Poor: 5.982s
            Ok: 22.099s
            Ideal: 43.748s
            Over: 0s
        Spin Time: 4.389s
            Imbalance or Serial Spinning: 4.389s
            Lock Contention: 0s
            Other: 0s
        Overhead Time: 0s
            Creation: 0s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 48
    Paused Time: 0s

Top Hotspots
Function                    Module              CPU Time
--------------------------  ------------------  --------
main._omp_fn.1              nbody_openmp_ultra   71.609s
gomp_team_barrier_wait_end  libgomp.so.1          3.371s
gomp_simple_barrier_wait    libgomp.so.1          0.389s
gomp_team_barrier_wait      libgomp.so.1          0.320s
gomp_simple_barrier_wait    libgomp.so.1          0.310s
[Others]                    N/A                   0.220s
Effective CPU Utilization: 36.9%
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Average Effective CPU Utilization: 17.707 out of 48
Collection and Platform Info
    Application Command Line: ./bin/nbody_openmp_ultra "10000" "1000" "bench" 
    Operating System: 3.10.0-1160.el7.x86_64 \S Kernel \r on an \m 
    Computer Name: cn052
    Result Size: 5.5 MB 
    Collection start time: 11:45:22 26/12/2025 UTC
    Collection stop time: 11:45:26 26/12/2025 UTC
    Collector Type: Driverless Perf per-process counting,User-mode sampling and tracing
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 2.893 GHz 
        Logical CPU Count: 48
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
