==========================================
SLURM_CLUSTER_NAME = paramutkarsh
SLURM_ARRAY_JOB_ID = 
SLURM_ARRAY_TASK_ID = 
SLURM_ARRAY_TASK_COUNT = 
SLURM_ARRAY_TASK_MAX = 
SLURM_ARRAY_TASK_MIN = 
SLURM_JOB_ACCOUNT = cdac
SLURM_JOB_ID = 40579
SLURM_JOB_NAME = prof_omp_u
SLURM_JOB_NODELIST = cn075
SLURM_JOB_USER = chuk355
SLURM_JOB_UID = 23411
SLURM_JOB_PARTITION = cpu
SLURM_TASK_PID = 24061
SLURM_SUBMIT_DIR = /home/chuk355/HPC-NBody
SLURM_CPUS_ON_NODE = 48
SLURM_NTASKS = 1
SLURM_TASK_PID = 24061
==========================================
============================================
Profiling OpenMP Ultra
Particles: 5000 | Threads: 48
Output: vtune_reports/openmp_ultra_N5000_ID40579
============================================
--- Topology Check ---
Threads: 48
Thread 0 is running on Core 0 (NUMA Split Check)
Thread 24 is running on Core 24 (NUMA Split Check)
Time: 0.732772 s
GFLOPs: 682.34
Elapsed Time: 3.450s
    CPU Time: 35.759s
        Effective Time: 32.469s
            Idle: 0s
            Poor: 3.000s
            Ok: 13.320s
            Ideal: 16.149s
            Over: 0s
        Spin Time: 3.290s
            Imbalance or Serial Spinning: 3.290s
            Lock Contention: 0s
            Other: 0s
        Overhead Time: 0s
            Creation: 0s
            Scheduling: 0s
            Reduction: 0s
            Atomics: 0s
            Other: 0s
    Total Thread Count: 48
    Paused Time: 0s

Top Hotspots
Function                    Module              CPU Time
--------------------------  ------------------  --------
main._omp_fn.1              nbody_openmp_ultra   32.439s
gomp_team_barrier_wait_end  libgomp.so.1          2.480s
gomp_simple_barrier_wait    libgomp.so.1          0.380s
gomp_simple_barrier_wait    libgomp.so.1          0.340s
gomp_team_barrier_wait      libgomp.so.1          0.090s
[Others]                    N/A                   0.030s
Effective CPU Utilization: 21.6%
 | The metric value is low, which may signal a poor logical CPU cores
 | utilization caused by load imbalance, threading runtime overhead, contended
 | synchronization, or thread/process underutilization. Explore sub-metrics to
 | estimate the efficiency of MPI and OpenMP parallelism or run the Locks and
 | Waits analysis to identify parallel bottlenecks for other parallel runtimes.
 |
    Average Effective CPU Utilization: 10.365 out of 48
Collection and Platform Info
    Application Command Line: ./bin/nbody_openmp_ultra "5000" "1000" "bench" 
    Operating System: 3.10.0-1160.el7.x86_64 \S Kernel \r on an \m 
    Computer Name: cn075
    Result Size: 4.6 MB 
    Collection start time: 11:45:32 26/12/2025 UTC
    Collection stop time: 11:45:35 26/12/2025 UTC
    Collector Type: Driverless Perf per-process counting,User-mode sampling and tracing
    CPU
        Name: Intel(R) Xeon(R) Processor code named Cascadelake
        Frequency: 2.893 GHz 
        Logical CPU Count: 48
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: available

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
